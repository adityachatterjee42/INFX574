{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1\n",
    "#1 --> solution in pdf of images, attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x->1\n",
      "f(x)->-1\n",
      "gradient->-2\n",
      "recomputed x->0.8\n",
      "x->0.8\n",
      "f(x)->-0.6400000000000001\n",
      "gradient->-1.6\n",
      "recomputed x->0.64\n",
      "x->0.64\n",
      "f(x)->-0.4096\n",
      "gradient->-1.28\n",
      "recomputed x->0.512\n",
      "x->0.512\n",
      "f(x)->-0.262144\n",
      "gradient->-1.024\n",
      "recomputed x->0.4096\n",
      "x->0.4096\n",
      "f(x)->-0.16777216\n",
      "gradient->-0.8192\n",
      "recomputed x->0.32768\n",
      "x->0.32768\n",
      "f(x)->-0.10737418240000002\n",
      "gradient->-0.65536\n",
      "recomputed x->0.26214400000000004\n",
      "x->0.26214400000000004\n",
      "f(x)->-0.06871947673600003\n",
      "gradient->-0.5242880000000001\n",
      "recomputed x->0.20971520000000005\n",
      "x->0.20971520000000005\n",
      "f(x)->-0.04398046511104002\n",
      "gradient->-0.4194304000000001\n",
      "recomputed x->0.16777216000000003\n",
      "x->0.16777216000000003\n",
      "f(x)->-0.02814749767106561\n",
      "gradient->-0.33554432000000006\n",
      "recomputed x->0.13421772800000004\n",
      "x->0.13421772800000004\n",
      "f(x)->-0.018014398509481992\n",
      "gradient->-0.26843545600000007\n",
      "recomputed x->0.10737418240000003\n",
      "x->0.10737418240000003\n",
      "f(x)->-0.011529215046068476\n",
      "gradient->-0.21474836480000006\n",
      "recomputed x->0.08589934592000002\n",
      "x->0.08589934592000002\n",
      "f(x)->-0.0073786976294838245\n",
      "gradient->-0.17179869184000005\n",
      "recomputed x->0.06871947673600001\n",
      "x->0.06871947673600001\n",
      "f(x)->-0.004722366482869647\n",
      "gradient->-0.13743895347200002\n",
      "recomputed x->0.05497558138880001\n",
      "x->0.05497558138880001\n",
      "f(x)->-0.003022314549036574\n",
      "gradient->-0.10995116277760002\n",
      "recomputed x->0.04398046511104001\n",
      "x->0.04398046511104001\n",
      "f(x)->-0.0019342813113834073\n",
      "gradient->-0.08796093022208001\n",
      "recomputed x->0.035184372088832\n",
      "x->0.035184372088832\n",
      "f(x)->-0.0012379400392853804\n",
      "gradient->-0.070368744177664\n",
      "recomputed x->0.028147497671065603\n",
      "x->0.028147497671065603\n",
      "f(x)->-0.0007922816251426435\n",
      "gradient->-0.056294995342131206\n",
      "recomputed x->0.02251799813685248\n",
      "maxima reached in 17 iterations!\n",
      "--------------------------------------------------\n",
      "x->2\n",
      "f(x)->-4\n",
      "gradient->-4\n",
      "recomputed x->1.2\n",
      "x->1.2\n",
      "f(x)->-1.44\n",
      "gradient->-2.4\n",
      "recomputed x->0.72\n",
      "x->0.72\n",
      "f(x)->-0.5184\n",
      "gradient->-1.44\n",
      "recomputed x->0.432\n",
      "x->0.432\n",
      "f(x)->-0.18662399999999998\n",
      "gradient->-0.864\n",
      "recomputed x->0.2592\n",
      "x->0.2592\n",
      "f(x)->-0.06718463999999999\n",
      "gradient->-0.5184\n",
      "recomputed x->0.15552\n",
      "x->0.15552\n",
      "f(x)->-0.024186470399999997\n",
      "gradient->-0.31104\n",
      "recomputed x->0.09331199999999999\n",
      "x->0.09331199999999999\n",
      "f(x)->-0.008707129343999998\n",
      "gradient->-0.18662399999999998\n",
      "recomputed x->0.055987199999999994\n",
      "x->0.055987199999999994\n",
      "f(x)->-0.0031345665638399995\n",
      "gradient->-0.11197439999999999\n",
      "recomputed x->0.033592319999999995\n",
      "x->0.033592319999999995\n",
      "f(x)->-0.0011284439629823996\n",
      "gradient->-0.06718463999999999\n",
      "recomputed x->0.020155391999999994\n",
      "x->0.020155391999999994\n",
      "f(x)->-0.0004062398266736638\n",
      "gradient->-0.04031078399999999\n",
      "recomputed x->0.012093235199999997\n",
      "x->0.012093235199999997\n",
      "f(x)->-0.00014624633760251896\n",
      "gradient->-0.024186470399999993\n",
      "recomputed x->0.007255941119999998\n",
      "x->0.007255941119999998\n",
      "f(x)->-5.264868153690683e-05\n",
      "gradient->-0.014511882239999996\n",
      "recomputed x->0.004353564671999998\n",
      "x->0.004353564671999998\n",
      "f(x)->-1.895352535328645e-05\n",
      "gradient->-0.008707129343999996\n",
      "recomputed x->0.0026121388031999987\n",
      "x->0.0026121388031999987\n",
      "f(x)->-6.823269127183122e-06\n",
      "gradient->-0.005224277606399997\n",
      "recomputed x->0.0015672832819199991\n",
      "maxima reached in 14 iterations!\n",
      "--------------------------------------------------\n",
      "x->1\n",
      "f(x)->-1\n",
      "gradient->-2\n",
      "recomputed x->0.8\n",
      "x->0.8\n",
      "f(x)->-0.6400000000000001\n",
      "gradient->-1.6\n",
      "recomputed x->0.64\n",
      "x->0.64\n",
      "f(x)->-0.4096\n",
      "gradient->-1.28\n",
      "recomputed x->0.512\n",
      "x->0.512\n",
      "f(x)->-0.262144\n",
      "gradient->-1.024\n",
      "recomputed x->0.4096\n",
      "x->0.4096\n",
      "f(x)->-0.16777216\n",
      "gradient->-0.8192\n",
      "recomputed x->0.32768\n",
      "x->0.32768\n",
      "f(x)->-0.10737418240000002\n",
      "gradient->-0.65536\n",
      "recomputed x->0.26214400000000004\n",
      "x->0.26214400000000004\n",
      "f(x)->-0.06871947673600003\n",
      "gradient->-0.5242880000000001\n",
      "recomputed x->0.20971520000000005\n",
      "x->0.20971520000000005\n",
      "f(x)->-0.04398046511104002\n",
      "gradient->-0.4194304000000001\n",
      "recomputed x->0.16777216000000003\n",
      "x->0.16777216000000003\n",
      "f(x)->-0.02814749767106561\n",
      "gradient->-0.33554432000000006\n",
      "recomputed x->0.13421772800000004\n",
      "x->0.13421772800000004\n",
      "f(x)->-0.018014398509481992\n",
      "gradient->-0.26843545600000007\n",
      "recomputed x->0.10737418240000003\n",
      "x->0.10737418240000003\n",
      "f(x)->-0.011529215046068476\n",
      "gradient->-0.21474836480000006\n",
      "recomputed x->0.08589934592000002\n",
      "x->0.08589934592000002\n",
      "f(x)->-0.0073786976294838245\n",
      "gradient->-0.17179869184000005\n",
      "recomputed x->0.06871947673600001\n",
      "x->0.06871947673600001\n",
      "f(x)->-0.004722366482869647\n",
      "gradient->-0.13743895347200002\n",
      "recomputed x->0.05497558138880001\n",
      "x->0.05497558138880001\n",
      "f(x)->-0.003022314549036574\n",
      "gradient->-0.10995116277760002\n",
      "recomputed x->0.04398046511104001\n",
      "x->0.04398046511104001\n",
      "f(x)->-0.0019342813113834073\n",
      "gradient->-0.08796093022208001\n",
      "recomputed x->0.035184372088832\n",
      "x->0.035184372088832\n",
      "f(x)->-0.0012379400392853804\n",
      "gradient->-0.070368744177664\n",
      "recomputed x->0.028147497671065603\n",
      "x->0.028147497671065603\n",
      "f(x)->-0.0007922816251426435\n",
      "gradient->-0.056294995342131206\n",
      "recomputed x->0.02251799813685248\n",
      "x->0.02251799813685248\n",
      "f(x)->-0.0005070602400912918\n",
      "gradient->-0.04503599627370496\n",
      "recomputed x->0.018014398509481985\n",
      "x->0.018014398509481985\n",
      "f(x)->-0.00032451855365842676\n",
      "gradient->-0.03602879701896397\n",
      "recomputed x->0.014411518807585589\n",
      "x->0.014411518807585589\n",
      "f(x)->-0.00020769187434139315\n",
      "gradient->-0.028823037615171177\n",
      "recomputed x->0.01152921504606847\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2 and 3\n",
    "'''\n",
    "\n",
    "#defining function from assignment as well as the 1D gradient defined as partial derivative\n",
    "def f(x):\n",
    "    return -(x*x)\n",
    "\n",
    "def gradient(x):\n",
    "    return -(2*x)\n",
    "\n",
    "#iterative gradient ascent function\n",
    "def gradientAscent(x0, r, iterations, e):\n",
    "    x=x0\n",
    "    iterCount=1\n",
    "    while(iterCount<=iterations):\n",
    "        print(\"x->{0}\".format(x))\n",
    "        print(\"f(x)->{0}\".format(f(x)))\n",
    "        print(\"gradient->{0}\".format(gradient(x)))\n",
    "        x = x + r * gradient(x)\n",
    "        print(\"recomputed x->{0}\".format(x))\n",
    "        if(abs(gradient(x))<e):\n",
    "            print(\"maxima reached in {0} iterations!\".format(iterCount))\n",
    "            break\n",
    "        iterCount+=1\n",
    "\n",
    "#running gradient ascent functions with various starting paramters\n",
    "gradientAscent(x0=1, r=0.1, iterations=20, e=0.05)\n",
    "print(\"--------------------------------------------------\")\n",
    "gradientAscent(x0=2, r=0.2, iterations=20, e=0.005)\n",
    "print(\"--------------------------------------------------\")\n",
    "gradientAscent(x0=1, r=0.1, iterations=20, e=0.0005)\n",
    "print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4,5,6 --> solution in pdf of images, attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x->[ 2 -3]\n",
      "f(x)->-52\n",
      "gradient->[ 8 40]\n",
      "recomputed x->[ 2.8  1. ]\n",
      "x->[ 2.8  1. ]\n",
      "f(x)->-27.04\n",
      "gradient->[ -9.6 -27.2]\n",
      "recomputed x->[ 1.84 -1.72]\n",
      "x->[ 1.84 -1.72]\n",
      "f(x)->-14.393600000000005\n",
      "gradient->[  3.2   20.16]\n",
      "recomputed x->[ 2.16   0.296]\n",
      "x->[ 2.16   0.296]\n",
      "f(x)->-7.923968000000005\n",
      "gradient->[ -5.504 -13.376]\n",
      "recomputed x->[ 1.6096 -1.0416]\n",
      "x->[ 1.6096 -1.0416]\n",
      "f(x)->-4.564019200000003\n",
      "gradient->[  0.9472  10.2272]\n",
      "recomputed x->[ 1.70432 -0.01888]\n",
      "x->[ 1.70432 -0.01888]\n",
      "f(x)->-2.7788480512000016\n",
      "gradient->[-3.33312 -6.5152 ]\n",
      "recomputed x->[ 1.371008 -0.6704  ]\n",
      "x->[ 1.371008 -0.6704  ]\n",
      "f(x)->-1.7986571632640012\n",
      "gradient->[-0.060416  5.242368]\n",
      "recomputed x->[ 1.3649664 -0.1461632]\n",
      "x->[ 1.3649664 -0.1461632]\n",
      "f(x)->-1.2360112937369607\n",
      "gradient->[-2.14528   -3.1212544]\n",
      "recomputed x->[ 1.1504384  -0.45828864]\n",
      "x->[ 1.1504384  -0.45828864]\n",
      "f(x)->-0.8948049336598534\n",
      "gradient->[-0.46772224  2.73086464]\n",
      "recomputed x->[ 1.10366618 -0.18520218]\n",
      "x->[ 1.10366618 -0.18520218]\n",
      "f(x)->-0.6748722865163473\n",
      "gradient->[-1.46652365 -1.45142989]\n",
      "recomputed x->[ 0.95701381 -0.33034516]\n",
      "x->[ 0.95701381 -0.33034516]\n",
      "f(x)->-0.5243193173745024\n",
      "gradient->[-0.59264696  1.45746739]\n",
      "recomputed x->[ 0.89774911 -0.18459843]\n",
      "x->[ 0.89774911 -0.18459843]\n",
      "f(x)->-0.41567381037724727\n",
      "gradient->[-1.05710453 -0.63742165]\n",
      "recomputed x->[ 0.79203866 -0.24834059]\n",
      "x->[ 0.79203866 -0.24834059]\n",
      "f(x)->-0.33392823730137033\n",
      "gradient->[-0.59071496  0.8052948 ]\n",
      "recomputed x->[ 0.73296717 -0.16781111]\n",
      "x->[ 0.73296717 -0.16781111]\n",
      "f(x)->-0.27052528050728825\n",
      "gradient->[-0.79468989 -0.2468909 ]\n",
      "recomputed x->[ 0.65349818 -0.1925002 ]\n",
      "x->[ 0.65349818 -0.1925002 ]\n",
      "f(x)->-0.22031636427097925\n",
      "gradient->[-0.53699555  0.46601049]\n",
      "recomputed x->[ 0.59979862 -0.14589915]\n",
      "x->[ 0.59979862 -0.14589915]\n",
      "f(x)->-0.18001044594190735\n",
      "gradient->[-0.61600064 -0.06480807]\n",
      "recomputed x->[ 0.53819856 -0.15237996]\n",
      "x->[ 0.53819856 -0.15237996]\n",
      "f(x)->-0.14737220583792462\n",
      "gradient->[-0.46687728  0.2852851 ]\n",
      "recomputed x->[ 0.49151083 -0.12385145]\n",
      "x->[ 0.49151083 -0.12385145]\n",
      "f(x)->-0.12079903304825522\n",
      "gradient->[-0.48761587  0.01557985]\n",
      "recomputed x->[ 0.44274924 -0.12229346]\n",
      "x->[ 0.44274924 -0.12229346]\n",
      "f(x)->-0.099091068112483\n",
      "gradient->[-0.39632463  0.18569844]\n",
      "recomputed x->[ 0.40311678 -0.10372362]\n",
      "x->[ 0.40311678 -0.10372362]\n",
      "f(x)->-0.08132092597191219\n",
      "gradient->[-0.39133908  0.04711079]\n",
      "recomputed x->[ 0.36398287 -0.09901254]\n",
      "x->[ 0.36398287 -0.09901254]\n",
      "f(x)->-0.06675592078405106\n",
      "gradient->[-0.33191558  0.12826916]\n",
      "recomputed x->[ 0.33079131 -0.08618562]\n",
      "x->[ 0.33079131 -0.08618562]\n",
      "f(x)->-0.05480876415685216\n",
      "gradient->[-0.31684013  0.05580474]\n",
      "recomputed x->[ 0.2991073  -0.08060515]\n",
      "x->[ 0.2991073  -0.08060515]\n",
      "f(x)->-0.045004343533897546\n",
      "gradient->[-0.275794    0.09325321]\n",
      "recomputed x->[ 0.2715279  -0.07127983]\n",
      "x->[ 0.2715279  -0.07127983]\n",
      "f(x)->-0.03695606366609811\n",
      "gradient->[-0.25793648  0.05436567]\n",
      "recomputed x->[ 0.24573425 -0.06584326]\n",
      "x->[ 0.24573425 -0.06584326]\n",
      "f(x)->-0.030348224927765073\n",
      "gradient->[-0.22809546  0.07055519]\n",
      "recomputed x->[ 0.22292471 -0.05878774]\n",
      "x->[ 0.22292471 -0.05878774]\n",
      "f(x)->-0.024922453227016108\n",
      "gradient->[-0.21069844  0.04890507]\n",
      "recomputed x->[ 0.20185486 -0.05389724]\n",
      "x->[ 0.20185486 -0.05389724]\n",
      "f(x)->-0.02046700532924869\n",
      "gradient->[-0.18812078  0.05493633]\n",
      "recomputed x->[ 0.18304278 -0.0484036 ]\n",
      "x->[ 0.18304278 -0.0484036 ]\n",
      "f(x)->-0.01680821019282247\n",
      "gradient->[-0.17247116  0.04228651]\n",
      "recomputed x->[ 0.16579567 -0.04417495]\n",
      "x->[ 0.16579567 -0.04417495]\n",
      "f(x)->-0.0138035520716654\n",
      "gradient->[-0.15489153  0.04361656]\n",
      "recomputed x->[ 0.15030652 -0.0398133 ]\n",
      "x->[ 0.15030652 -0.0398133 ]\n",
      "f(x)->-0.011336045839618244\n",
      "gradient->[-0.14135985  0.03578668]\n",
      "recomputed x->[ 0.13617053 -0.03623463]\n",
      "x->[ 0.13617053 -0.03623463]\n",
      "f(x)->-0.009309645579741933\n",
      "gradient->[-0.12740255  0.03507193]\n",
      "recomputed x->[ 0.12343028 -0.03272744]\n",
      "x->[ 0.12343028 -0.03272744]\n",
      "f(x)->-0.00764548778935172\n",
      "gradient->[-0.11595081  0.02991786]\n",
      "recomputed x->[ 0.1118352  -0.02973565]\n",
      "x->[ 0.1118352  -0.02973565]\n",
      "f(x)->-0.006278813052481974\n",
      "gradient->[-0.10472779  0.02842961]\n",
      "recomputed x->[ 0.10136242 -0.02689269]\n",
      "x->[ 0.10136242 -0.02689269]\n",
      "f(x)->-0.005156441430757741\n",
      "gradient->[-0.09515408  0.02483335]\n",
      "recomputed x->[ 0.09184701 -0.02440935]\n",
      "x->[ 0.09184701 -0.02440935]\n",
      "f(x)->-0.004234700862303601\n",
      "gradient->[-0.0860566   0.02316162]\n",
      "recomputed x->[ 0.08324135 -0.02209319]\n",
      "x->[ 0.08324135 -0.02209319]\n",
      "f(x)->-0.003477726728202384\n",
      "gradient->[-0.07810993  0.02052567]\n",
      "recomputed x->[ 0.07543035 -0.02004062]\n",
      "x->[ 0.07543035 -0.02004062]\n",
      "f(x)->-0.002856065808027591\n",
      "gradient->[-0.07069821  0.01892857]\n",
      "recomputed x->[ 0.06836053 -0.01814777]\n",
      "x->[ 0.06836053 -0.01814777]\n",
      "f(x)->-0.0023455299987247234\n",
      "gradient->[-0.06413     0.01692214]\n",
      "recomputed x->[ 0.06194753 -0.01645555]\n",
      "x->[ 0.06194753 -0.01645555]\n",
      "f(x)->-0.0019262550433485342\n",
      "gradient->[-0.05807286  0.01549871]\n",
      "recomputed x->[ 0.05614025 -0.01490568]\n",
      "x->[ 0.05614025 -0.01490568]\n",
      "f(x)->-0.001581927569742418\n",
      "gradient->[-0.05265777  0.01392991]\n",
      "recomputed x->[ 0.05087447 -0.01351269]\n",
      "x->[ 0.05087447 -0.01351269]\n",
      "f(x)->-0.0012991503263044325\n",
      "gradient->[-0.04769818  0.01270516]\n",
      "recomputed x->[ 0.04610465 -0.01224217]\n",
      "x->[ 0.04610465 -0.01224217]\n",
      "f(x)->-0.0010669208981553564\n",
      "gradient->[-0.04324061  0.01145618]\n",
      "recomputed x->[ 0.04178059 -0.01109656]\n",
      "x->[ 0.04178059 -0.01109656]\n",
      "f(x)->-0.0008762036119107747\n",
      "gradient->[-0.03917496  0.01042254]\n",
      "recomputed x->[ 0.0378631 -0.0100543]\n",
      "x->[ 0.0378631 -0.0100543]\n",
      "f(x)->-0.0007195779678377888\n",
      "gradient->[-0.03550898  0.00941646]\n",
      "recomputed x->[ 0.0343122  -0.00911266]\n",
      "x->[ 0.0343122  -0.00911266]\n",
      "f(x)->-0.0005909499181072278\n",
      "gradient->[-0.03217377  0.00855372]\n",
      "recomputed x->[ 0.03109482 -0.00825729]\n",
      "x->[ 0.03109482 -0.00825729]\n",
      "f(x)->-0.0004853147562751825\n",
      "gradient->[-0.0291605   0.00773728]\n",
      "recomputed x->[ 0.02817877 -0.00748356]\n",
      "x->[ 0.02817877 -0.00748356]\n",
      "f(x)->-0.00039856239182652027\n",
      "gradient->[-0.02642331  0.00702183]\n",
      "recomputed x->[ 0.02553644 -0.00678137]\n",
      "x->[ 0.02553644 -0.00678137]\n",
      "f(x)->-0.00032731743304069205\n",
      "gradient->[-0.02394738  0.00635622]\n",
      "recomputed x->[ 0.0231417  -0.00614575]\n",
      "x->[ 0.0231417  -0.00614575]\n",
      "f(x)->-0.00026880785592157074\n",
      "gradient->[-0.0217004   0.00576522]\n",
      "recomputed x->[ 0.02097166 -0.00556923]\n",
      "x->[ 0.02097166 -0.00556923]\n",
      "f(x)->-0.00022075714924300592\n",
      "gradient->[-0.01966641  0.00522103]\n",
      "recomputed x->[ 0.01900502 -0.00504713]\n"
     ]
    }
   ],
   "source": [
    "#7,8,9,10,11,12,13\n",
    "#2D case\n",
    "x0 = np.array([2,-3])\n",
    "a = np.array([[1,2],[2,8]])\n",
    "\n",
    "\n",
    "def f(a,x):\n",
    "    return -1*np.matmul(np.transpose(x),np.matmul(a,x))\n",
    "\n",
    "def gradient(a,x):\n",
    "    return -2*(np.matmul(a,x))\n",
    "\n",
    "#iterative gradient ascent function\n",
    "def gradientAscent2D(x0, r, iterations, e):\n",
    "    x=x0\n",
    "    iterCount=1\n",
    "    while(iterCount<=iterations):\n",
    "        print(\"x->{0}\".format(x))\n",
    "        print(\"f(x)->{0}\".format(f(a,x)))\n",
    "        print(\"gradient->{0}\".format(gradient(a,x)))\n",
    "        x = x + r * gradient(a,x)\n",
    "        print(\"recomputed x->{0}\".format(x))\n",
    "        '''\n",
    "        if(abs(gradient(a,x))<e):\n",
    "            print(\"maxima reached in {0} iterations!\".format(iterCount))\n",
    "            break\n",
    "        '''\n",
    "        iterCount+=1\n",
    "\n",
    "#print(f(a,x0))\n",
    "#print(gradient(a,x0))\n",
    "\n",
    "gradientAscent2D(x0, 0.1, 50, 0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size-->10\n",
      "mean-->0.0141487037234\n",
      "standard deviation-->0.328167389307\n",
      "95% confidence interval-->(-0.0062258102629996846, 0.034523217709785003)\n",
      "-----------------------------------\n",
      "sample size-->1000\n",
      "mean-->0.00183356847885\n",
      "standard deviation-->0.0322041285783\n",
      "95% confidence interval-->(-0.00016584855501089717, 0.0038329855127062738)\n",
      "-----------------------------------\n",
      "sample size-->100000\n",
      "mean-->-4.78945872231e-05\n",
      "standard deviation-->0.00316104167354\n",
      "95% confidence interval-->(-0.00024415018691877912, 0.00014836101247258439)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "#PART 2\n",
    "#1,2,3,4\n",
    "\n",
    "def normalProperties(n):\n",
    "    mu = 0\n",
    "    sigma = 1\n",
    "    means = []\n",
    "    for repetition in range(1000):\n",
    "        ndist = np.random.normal(mu, sigma, n)\n",
    "        means.append(np.mean(ndist))\n",
    "    print(\"sample size-->\"+str(n))\n",
    "    print(\"mean-->\"+str(np.mean(means)))\n",
    "    print(\"standard deviation-->\"+str(np.std(means)))\n",
    "    #using interval function from scipy.stats instead of quantiles\n",
    "    print(\"95% confidence interval-->\"+str(st.t.interval(0.95, len(means)-1, loc=np.mean(means), scale=st.sem(means))))\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "for i in [10, 1000, 100000]:\n",
    "    normalProperties(i)\n",
    "    \n",
    "#as the values below indicate, the mean, and standard deviations decrease and the 95% CI tightens as our sample size increases\n",
    "#that is, our samples' properties become closer to an actual normal distribution with an increase in sample size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHOtJREFUeJzt3Xl8XXWd//HXJ1uTtGmWJl3oQlooYIUiJYCCCwJiZSkjOg59OI4LUH/j4OhPZrTqTxxREZeZ0d8DhCkIBX/S4iAKQ4sVWUSBKkW0tJQlQErTLaFJl7TZ8/n9cW5Kkt7k3iQ3Ofee+34+Hvdxzzn323s+p8u733zP95xj7o6IiERLTtgFiIhI6incRUQiSOEuIhJBCncRkQhSuIuIRJDCXUQkghTuIiIRpHAXEYkghbuISATlhbXjyspKr66uDmv3IiIZ6ZlnnnnD3asStQst3Kurq9mwYUNYuxcRyUhmtjWZdhqWERGJIIW7iEgEKdxFRCJI4S4iEkEKdxGRCFK4i4hEkMJdRCSCMi/cX3wRHngA6urCrkREJG1lXrj/+7/DxRfDunVhVyIikrYyL9wrKgD43l1PUr18TcjFiIikp4Thbma3mVmDmW1K0O40M+s2sw+nrrw4pkwBoLStZUx3IyKSyZLpua8EFg/VwMxyge8CYz9WEuu5l7UeGPNdiYhkqoTh7u6PA00Jmn0W+AXQkIqihtQb7m0KdxGRwYx6zN3MZgIfBG4efTlJUM9dRCShVJxQ/SHwJXfvTtTQzJaZ2QYz29DY2DiyvannLiKSUCru514DrDYzgErgAjPrcvdfDWzo7iuAFQA1NTU+or0dDnedUBURGcyow93d5/Yum9lK4IF4wZ4yfYdlfGT/P4iIRF3CcDezVcDZQKWZ1QNfB/IB3H18xtn7KiqiLa+Awq4OCrvax333IiKZIGG4u/vSZL/M3T8xqmqStLdwEtNbmihr1dCMiEg8mXeFKrC3sATQSVURkcFkZrgXKdxFRIaS0eFeqmEZEZG4MjPcY8My5a37Q65ERCQ9ZWi4TwI0111EZDAZGe77NOYuIjKkjAz33mGZUt1fRkQkrgwNdw3LiIgMJTPDXcMyIiJDyshw39d7EZOGZURE4srIcN9bFAzL6FF7IiLxZWS4NxdOBjQsIyIymIwM99b8CbTn5lHc2Q5tbWGXIyKSdjIy3DE7PO5OU6LHu4qIZJ/MDHfenOuucBcROVLmhnvspKrCXUTkSBkb7hqWEREZXMaGu4ZlREQGl7nhrmEZEZFBZW64q+cuIjKohOFuZreZWYOZbRrk84+a2cbY60kzOzn1ZR6p97a/CncRkSMl03NfCSwe4vPXgPe4+0Lgm8CKFNSVUHNvz33PnvHYnYhIRslL1MDdHzez6iE+f7LP6npg1ujLSmyveu4iIoNK9Zj75cCDKf7OuHrv6a5wFxE5UsKee7LM7L0E4f7OIdosA5YBzJkzZ1T705i7iMjgUtJzN7OFwK3AJe4+6CC4u69w9xp3r6mqqhrVPjVbRkRkcKMOdzObA9wLfMzdXxp9SclpKSiiy3KgpQU6OsZrtyIiGSHhsIyZrQLOBirNrB74OpAP4O43A9cAU4AfmxlAl7vXjFXBfQpjb1EJlYf2QXMzTJs25rsUEckUycyWWZrg8yuAK1JW0TDsK4yFe1OTwl1EpI+MvUIVNGNGRGQwmR3uRbqQSUQknmiEu3ruIiL9ZHa46xYEIiJxZXS47ykuDRYaGsItREQkzWR0uDdOLA8Wdu0KtxARkTSjcBcRiaDMDvdJCncRkXgyOtwb1HMXEYkro8O9qbgUzKCxEbq6wi5HRCRtZHS4d+fkQlUVuAcBLyIiQIaHOwDTpwfvGpoRETlM4S4iEkEKdxGRCMr4cL/5xYMAfG/lY1QvXxNyNSIi6SHjw733Qqaqg80hVyIikj6iE+4tCncRkV7RCXf13EVEDsv4cG+YpHAXERko48NdPXcRkSMlDHczu83MGsxs0yCfm5n9XzOrNbONZrYo9WUObv+EibTn5lPS0UpRR9t47lpEJG0l03NfCSwe4vMPAPNjr2XATaMvaxjMDvfeKw/tHdddi4ikq4Th7u6PA0M9pPQS4E4PrAfKzGxGqgpMhmbMiIj0l4ox95nAtj7r9bFtRzCzZWa2wcw2NKbwRl+NOqkqItJPKsLd4mzzeA3dfYW717h7TVVVVQp2HWicWAYo3EVEeqUi3OuB2X3WZwE7UvC9SdOMGRGR/lIR7vcD/xCbNfN2YJ+770zB9yZN4S4i0l9eogZmtgo4G6g0s3rg60A+gLvfDKwFLgBqgUPAJ8eq2MEo3EVE+ksY7u6+NMHnDvxTyioaAYW7iEh/GX+FKvSZLdOiee4iIhCVcC/uM1vG407UERHJKpEI9/b8CeyfMJGCni5o1tCMiEgkwh3eHHfX4/ZERCIV7sHQDDvHdRamiEhailC4q+cuItJL4S4iEkGRCffdJRXBQn19uIWIiKSByIT79slTg4WtW8MtREQkDUQm3OtLpwULCncRkeiE++Gee11dqHWIiKSDyIR748Qy2vIKoKkJDhwIuxwRkVBFJtwxY/vk2ANANDQjIlkuOuGOTqqKiPSKVLjXlyrcRUQgYuGuk6oiIoFIhbt67iIigUiF+/ZS9dxFRCBi4V4/WRcyiYhAxMK9YVI55OXB7t3Q2hp2OSIioUkq3M1ssZm9aGa1ZrY8zudzzOxRM3vWzDaa2QWpLzWxnpxcmDMnWHn99TBKEBFJCwnD3cxygRuBDwALgKVmtmBAs/8D/NzdTwEuA36c6kKTdvTRwbuGZkQkiyXTcz8dqHX3V929A1gNXDKgjQOTY8ulwI7UlThMveGuk6oiksXykmgzE9jWZ70eOGNAm38DfmNmnwUmAufF+yIzWwYsA5jTO3ySatXVwfvWrVQvX3N4c931F47N/kRE0lAyPXeLs80HrC8FVrr7LOAC4KdmdsR3u/sKd69x95qqqqrhV5sM9dxFRJIK93pgdp/1WRw57HI58HMAd38KKAQqU1HgsPXpuYuIZKtkwv1pYL6ZzTWzAoITpvcPaPM6cC6Amb2FINwbU1lo0nRCVUQkcbi7exdwFbAO2EIwK2azmV1rZktiza4GrjSzvwKrgE+4+8Chm3Fx7I830m059NRvJ7+7M4wSRERCl8wJVdx9LbB2wLZr+iw/D5yV2tJGpis3j12TpjDzQCPTD+xhW9n0sEsSERl3kbpCtdf20uBk7ax9u0OuREQkHJEM996HZc/a1xByJSIi4YhkuPfe113hLiLZKpLhvrV8BgDVzeFdKCsiEqZIhvsrFbMAOHbPtgQtRUSiKZrhPiUI93lN2zHvCbkaEZHxF8lw3184iYaJ5RR1tTNzfzjXUomIhCmS4Q5QOyW4Y8Kxb2hoRkSyT2TDvXdo5pim+pArEREZf5EN996e+zE6qSoiWSgLwl09dxHJPpENd02HFJFsFtlw31UyhZaCIqa07qf80L6wyxERGVeRDXfMDvfedVJVRLJNdMMdqI3NmNF0SBHJNpEO91d6T6qq5y4iWSbi4a6TqiKSnZJ6ElOmqq14czpk9fI1h7fXXX9hWCWJiIyLSPfct5bPoDMnl1n7GpjQ2R52OSIi4ybS4d6Vm8fWshnk4Mxr3h52OSIi4yapcDezxWb2opnVmtnyQdp8xMyeN7PNZnZXasscuVc0Y0ZEslDCcDezXOBG4APAAmCpmS0Y0GY+8GXgLHd/K/D5Mah1RA7fHVK3IRCRLJJMz/10oNbdX3X3DmA1cMmANlcCN7p7M4C7p83DS1+qnAPACY2vhVyJiMj4SSbcZwJ9xzTqY9v6Og44zsyeMLP1ZrY43heZ2TIz22BmGxobx+chGpumHwvASbtqx2V/IiLpIJlwtzjbfMB6HjAfOBtYCtxqZmVH/CL3Fe5e4+41VVVVw611RF6tmElLQRFHHXiDyoPN47JPEZGwJRPu9cDsPuuzgB1x2tzn7p3u/hrwIkHYh84th83TjgHgRPXeRSRLJBPuTwPzzWyumRUAlwH3D2jzK+C9AGZWSTBM82oqCx2NjbGhmYUKdxHJEgnD3d27gKuAdcAW4OfuvtnMrjWzJbFm64A9ZvY88Cjwr+6+Z6yKHq7nNO4uIlkmqdsPuPtaYO2Abdf0WXbgC7FX2nluejBCdNKul0OuRERkfET6CtVedeUz2F9QzPSWJqpamsIuR0RkzGVFuLvlsHl6cFJVQzMikg2yItwBNsaGZhZqaEZEskDWhPumaeq5i0j2yJpw3zij96RqLfjAa7BERKIla8J9a9kM9k+YyNSDzbBj4DVYIiLRkjXhjhnPxU6qsmFDuLWIiIyx7Al33pzvzjPPhFuIiMgYy6pw/8uM44KFJ54ItxARkTGWVeH+p9knBgtPPgnteqaqiERXVoV7U3EpL1QeDW1tsH592OWIiIyZrAp3gKeOXhgsPPpouIWIiIyhrAv39XNOChYU7iISYdkX7rNPArNgWKa1NexyRETGRNaF+76iEjj5ZOjoCE6siohEUNaFOwDnnBO8a2hGRCIqO8P9ve8N3hXuIhJR2Rnu73oX5OTAn/4ELS1hVyMiknLZGe6lpXDqqdDVpatVRSSSkgp3M1tsZi+aWa2ZLR+i3YfNzM2sJnUljhENzYhIhCUMdzPLBW4EPgAsAJaa2YI47UqAfwb+mOoix8S55wbva9cO3U5EJAMl03M/Hah191fdvQNYDVwSp903ge8BbSmsb0xUL1/D/IcOsb+gGJ57Dl55JeySRERSKplwnwls67NeH9t2mJmdAsx29wdSWNuY6szN55FjTwPg21dcR/XyNSFXJCKSOsmEu8XZdvg5dWaWA/wncHXCLzJbZmYbzGxDY2Nj8lWOkXXz3wHA+19+KuRKRERSK5lwrwdm91mfBfR9Tl0JcCLwmJnVAW8H7o93UtXdV7h7jbvXVFVVjbzqFPndvFNpz81n0fYXqGppCrscEZGUSSbcnwbmm9lcMysALgPu7/3Q3fe5e6W7V7t7NbAeWOLuaf8su0MFRTw+9xRycM5/WbcAFpHoSBju7t4FXAWsA7YAP3f3zWZ2rZktGesCx9pveodmXtLQjIhER14yjdx9LbB2wLZrBml79ujLGj+/PfZ0ui2Hd7y+EfbuhbKysEsSERm17LxCtY/m4lL+NPut5Pd0wxrNmBGRaMj6cIc3Z81wzz3hFiIikiIKd2DNCe+ky3LggQdg9+6wyxERGTWFO9A4qYJHjzktuJHYnXeGXY6IyKgp3GNWn3x+sHDrreA+dGMRkTSncI95bF4NzJgBL70Ef/hD2OWIiIyKwj2mOycXPvnJYOXWW8MtRkRklBTufbz7jbkAtN51dzDnXUQkQync+3i9fAZPHL2Qoq52WLUq7HJEREZM4T7A6oXvDxZuvBF6eoDg/u+9LxGRTKBwH+DXx5/JjpJK2LwZ7r8/8S8QEUlDCvcBOnPzWXH6pQD85TNfpPpLGfP8ERGRwxTucaw++XzeKC7lbTtf5p11fwm7HBGRYVO4x9GWX8htNcFjYq966u6QqxERGT6F+yB+uuhC9k+YyNu3beLU+ufDLkdEZFgU7oM4MGEiKxddBMDnnlilWxKISEZRuA/h9pol7J8wkXfXPcs5rzwddjkiIklTuA+hubiUH521FICvPXILBV2dIVckIpIchXsCdyy6iJenzGZu804++cx9YZcjIpIUhXsCXbl5XHvulQB89sm7YefOkCsSEUksqXA3s8Vm9qKZ1ZrZ8jiff8HMnjezjWb2sJkdnfpSw/P7uYt46NgzmNTRyr3v+6huRSAiaS9huJtZLnAj8AFgAbDUzBYMaPYsUOPuC4F7gO+lutCwffOcK2jLK+DSzY/y/peeDLscEZEhJdNzPx2odfdX3b0DWA1c0reBuz/q7odiq+uBWaktM3yvl8/gO2cH93v/zq9voKqlKeSKREQGl0y4zwS29Vmvj20bzOXAg6MpKl3duehCHq8+hYrW/fxg7Q/7zX3XnSNFJJ0kE+4WZ1vcK3rM7O+BGuD7g3y+zMw2mNmGxsbG5KtME245/MsFn6e5sIT3vPZnuOGGsEsSEYkrmXCvB2b3WZ8F7BjYyMzOA74KLHH39nhf5O4r3L3G3WuqqqpGUm/oGkqmsHzxZ4OVq6+Gxx8PtyARkTiSCfengflmNtfMCoDLgH43OjezU4D/Igj2htSXmV7WHX8mt526BDo74dJL4ZVXwi5JRKSfhOHu7l3AVcA6YAvwc3ffbGbXmtmSWLPvA5OA/zazv5hZ5J9y8a1zLocLLoA9e+Cii5jc1hJ2SSIih+Ul08jd1wJrB2y7ps/yeSmuK+315OQGz1k96yzYtImbWq/jUx/6Ou35E8IuTUREV6iOyuTJ8D//A1OnctbWjdxy77eY0Bn3dIOIyLhSuI9WdTU88ghvFJfy7rpnueXeb0Fra+JfpqmTIjKGFO6p8Na3svSy62gsLuPddc/CxRfDvn1hVyUiWSypMXeJr1+vu+poli69jlWrvkLVww/DmWcGQzbz5oVXoIhkLfXcU6i2cg4f/NgPYMECeP55OOMM+P3vwy5LRLKQwj3F6sumw5NPwuLF8MYbcM458J3vQHd32KWJSBZRuI+B6u/8gWNO+kdWnPZB6OqCr3wFzj0Xtm1L/ItFRFJA4T5GunNyue6cy+HBB2HaNPjd79g//y187fx/ZN4XI3+Nl4iETOE+1hYvho0b4eKLmdx+kG8+dDO/+unVLNz5UtzmmiIpIqmgcB8PU6fCfffx6b/5CjtKKlm4q5b77/wCN9z3XebtqQ+7OhGJIIX7eDFj3fFnct4VN3HzGR+iPTefi174PQ/95DPwqU/Bli1hVygiEaJwH2eHCoq4/uxP8p5lt3DXyYuDG+PffnswffKii3jH1r/2ewiIiMhI6CKmkOyaXMlXFl/Ff51xKb/L+TOsXAlr1rCKNbxafhR3n3w+7K4JTsaKiAyTeu4h21p+FNx0E7z+OnzjG+yaVMG85h18+bGVMHMmnH8+/OQn0BT/ma06ASsi8ajnPsaSDt2qKrjmGs5qeRvvefUZlv51He+rewYeeih4ffrT8K53BfetufBCOO44sHhPQBxeTXXXXzii7xCR9Kaee5rpzsnlkWNP58oPfQ127YJbboHzYrfLf+yx4NF+J5wAc+bAJz7BpZseZvbeXRqnF5F+1HNPZ1OmwBVXBK/mZli3Dh54IHivr4c77uA/Yk0bJpbDy2fDaadBTQ2ceipUVIRZvYiESOGeKcrL4bLLgldPD2zaBA8/zEM/vptF27cw9WAz/PKXwavXrFlw0knBa8GCoMf/lreEdwwiMm4U7pkoJwcWLoSFC7ly93HgztzmHSza/gIn7q5l4c6XWdDwGkX19UEP/8EH+/3yp4vL2Fo+g63lM6Dw6eCBI9XVMHt28B/CBD0qUCTTKdzTWNInPs14rWImr1XM5BcnnQtATk83c/bu4oTGOo5v3Mqxe7ZxTFM985q2U3VoL1WH9lKzfQtseuTI75s6NZipM2MGHHUUTJ8eTMmcNi34rLIyOAFcUQF5I/srpJO6ImMrqX+ZZrYY+BGQC9zq7tcP+HwCcCdwKrAH+Dt3r0ttqTIcPTm51FXMpK5iJr8+/qzD2817mHagieq9Ozi6eSffPXUy1NUFr23bYPt2aGgIXs8+m3hHpaXBuYGKCigrg/Jy7nrpAPsnFHNgwkT+9SOnB8+anTwZSkoOv2bsb+RgQRGteqC4yJgwTzDLwsxygZeA9wH1wNPAUnd/vk+bzwAL3f1/mdllwAfd/e+G+t6amhrfsGHDiIrO9jndfXu6qf69yOnppupgM1NbmpnW0sS0lj18+8ypsHs37NrFn55+kYpD+6lo3U9Z6wFySMEsnbw8mDgRiouD96KiYLmoKHgVFvZfLiwMho7ivQoKYMIErly9kc6cPDpy87jrM+8Ktufns/jGp+jMyaMrN5eunDye+Nr7g/3n5/d/z80d1lTT4f4kop9cZKTM7Bl3r0nULpme++lArbu/Gvvi1cAlwPN92lwC/Fts+R7gBjMzT/Q/h6SdnpxcdpdUsrukkudi27799TfD5yN9Qsm8h8ltBylv3U9pW8vh1+T2g5S0H2JyewuT2luZ1HGIkvaDvG9WMbS0wIED7Nz+BsWdbRR3tpHf1RU8czaFz529pe/K3W8u/npgw5uH+JLc3DeDPi+v/3Jubr/Xb/e00p2TQ4/lwG++1v/znJz+yzk53F67hx7LoccMXrjl8PaEL7P+7/G2mSW/nMoXJN4+kuVeAz8fzfawl8vKgkdxjqFkwn0m0PcpE/XAGYO1cfcuM9sHTAHeSEWREq7Bfjpwy2FfUQn7ikpG9f353Z0UdbZT3NFGUVc7hV3tFHW2M6Grg8KuDgo72yns6qCgu5PCrg6+cf4x0N4evNraoKMD2tv57ydeIb+nk/zuLgq6O8nv7ia/p5OzZpVAZyd0dvLCtibye7rJ6+kir7ubmZPyDn928FA7eT3d5PV0k+s9wdOzknyC1rF9VxrrErZ/b9+V2mH8Zkk0nHEGrF8/prtIJtzj/Ww6sEeeTBvMbBmwLLbaYmYvJrH/eCqJzn8cwz4W++4YVTJ64/Lncu1vh/kLtg7x2YFBP4nK37GoHAdE6Vj++MdKzEZ6LEcn0yiZcK8HZvdZnwXsGKRNvZnlAaXAETdDcfcVwIpkChuKmW1IZswpE+hY0lNUjiUqxwE6luFK5vYDTwPzzWyumRUAlwEDnxN3P/Dx2PKHgUc03i4iEp6EPffYGPpVwDqCqZC3uftmM7sW2ODu9wM/AX5qZrUEPfbLxrJoEREZWlLz3N19LbB2wLZr+iy3AX+b2tKGNOqhnTSiY0lPUTmWqBwH6FiGJeE8dxERyTy65a+ISARlVLib2Wwze9TMtpjZZjP7XNg1jYaZ5ZrZs2b2QNi1jIaZlZnZPWb2QuzP5h1h1zRSZva/Y3+3NpnZKjMrDLumZJnZbWbWYGab+myrMLOHzOzl2Ht5mDUma5Bj+X7s79hGM/ulmZWFWWOy4h1Ln8/+xczczCpTvd+MCnegC7ja3d8CvB34JzNbEHJNo/E5YEvYRaTAj4Bfu/sJwMlk6DGZ2Uzgn4Eadz+RYAJBJk0OWAksHrBtOfCwu88HHo6tZ4KVHHksDwEnuvtCgluifHm8ixqhlRx5LJjZbILburw+FjvNqHB3953u/ufY8gGCEJkZblUjY2azgAuBW8OuZTTMbDLwboIZU7h7h7vvDbeqUckDimLXaxRz5DUdacvdH+fI60suAe6ILd8B/M24FjVC8Y7F3X/j7l2x1fUE19ykvUH+XAD+E/gicS74TIWMCve+zKwaOAX4Y7iVjNgPCf5ge8IuZJTmAY3A7bEhplvNbGLYRY2Eu28HfkDQk9oJ7HP334Rb1ahNc/edEHSOgKkh15MqnwIeTNgqTZnZEmC7u/91rPaRkeFuZpOAXwCfd/f9YdczXGZ2EdDg7s+EXUsK5AGLgJvc/RTgIJnzo38/sfHoS4C5wFHARDP7+3CrkoHM7KsEQ7Q/C7uWkTCzYuCrwDWJ2o5GxoW7meUTBPvP3P3esOsZobOAJWZWB6wGzjGz/xduSSNWD9S7e+9PUPcQhH0mOg94zd0b3b0TuBcY21v3jb3dZjYDIPbeEHI9o2JmHwcuAj6awVfBH0PQgfhrLANmAX82s+mp3ElGhbuZGcHY7hZ3/49E7dOVu3/Z3We5ezXBCbtH3D0je4juvgvYZmbHxzadS//bQWeS14G3m1lx7O/auWToyeE++t4a5OPAfSHWMiqxhwZ9CVji7ofCrmek3P05d5/q7tWxDKgHFsX+LaVMRoU7QY/3YwQ93b/EXheEXZTwWeBnZrYReBtwXcj1jEjsp497gD8DzxH8+8iYqyLNbBXwFHC8mdWb2eXA9cD7zOxlgpkZ1w/1HelikGO5ASgBHor92x/qbvxpY5BjGfv9Zu5PNiIiMphM67mLiEgSFO4iIhGkcBcRiSCFu4hIBCncRUQiSOEuIhJBCncRkQhSuIuIRND/B+xroYCrUwz0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe5fa910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#5\n",
    "a, m = 3., 2.  # shape and mode\n",
    "s = (np.random.pareto(a, 1000) + 1) * m\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "count, bins, _ = plt.hist(s, 100, normed=True)\n",
    "fit = a*m**a / bins**(a+1)\n",
    "plt.plot(bins, max(count)*fit/max(fit), linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size-->10\n",
      "mean-->4.58808508927\n",
      "standard deviation-->18.9121313182\n",
      "95% confidence interval-->(3.4139114918605724, 5.7622586866783561)\n",
      "-----------------------------------\n",
      "sample size-->1000\n",
      "mean-->27.9200402103\n",
      "standard deviation-->573.77578781\n",
      "95% confidence interval-->(-7.7032520322520597, 63.543332452893942)\n",
      "-----------------------------------\n",
      "sample size-->100000\n",
      "mean-->18.9124402886\n",
      "standard deviation-->298.646545127\n",
      "95% confidence interval-->(0.37074964858462067, 37.454130928629489)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#6,7,8,9,10\n",
    "\n",
    "def paretoProperties(n):\n",
    "    mu = 0\n",
    "    sigma = 1\n",
    "    means = []\n",
    "    for repetition in range(1000):\n",
    "        a, m = 1, 0.5  # shape and mode\n",
    "        pdist = (np.random.pareto(a, n) + 1) * m\n",
    "        means.append(np.mean(pdist))\n",
    "    print(\"sample size-->\"+str(n))\n",
    "    print(\"mean-->\"+str(np.mean(means)))\n",
    "    print(\"standard deviation-->\"+str(np.std(means)))\n",
    "    #using interval function from scipy.stats instead of quantiles\n",
    "    print(\"95% confidence interval-->\"+str(st.t.interval(0.95, len(means)-1, loc=np.mean(means), scale=st.sem(means))))\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "for i in [10, 1000, 100000]:\n",
    "    paretoProperties(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
